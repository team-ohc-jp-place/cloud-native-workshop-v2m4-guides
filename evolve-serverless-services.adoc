= Lab3 - サーバーレスでサービスを進化
:experimental:

Cloud Native アプリケーション・アーキテクチャでは、現在、複数のマイクロサービスを *reactive* システムで運用しています。しかし、アプリケーションやサービスが24時間稼働している必要はありません。何かがサービスを利用する必要があるときに、*オンデマンド* で動作していればいいのです。これが *serverless* アーキテクチャが人気を集めている理由の一つです。

* *Serverless* はしばしば _FaaS_ (Functions-as-a-Service)という用語と互換的に使われます。しかし、サーバーレスはサーバーがないという意味ではありません。実際には、サーバーは存在します - パブリッククラウドプロバイダーは、アプリケーションを展開、実行、管理するサーバーを提供します。
* *Serverless computing* は開発者がソフトウェアシステムを構築して提供する方法に変化をもたらす新興のカテゴリーです。アプリケーション・インフラストラクチャをコードから切り離すことで、開発プロセスを大幅に簡素化しながら、新たなコストと効率性のメリットをもたらすことができます。サーバーレスコンピューティングと FaaS は、Cloud Native サービスや https://enterprisersproject.com/hybrid-cloud[ハイブリッドクラウド^] とともに、エンタープライズ IT の次の時代を定義する上で重要な役割を果たすことになるでしょう。
* *Serverless platforms* は API を提供しており、ユーザーがコードスニペット (_actions_ とも呼ばれる関数) を実行し、各関数の結果を返すことができます。サーバレスプラットフォームは、開発者が関数の結果を取得するためのエンドポイントも提供しています。これらのエンドポイントは、他の関数の入力として使用することができ、それによって関連する関数のシーケンス(またはチェーン)を提供することができます。

サーバーレスアプリケーションは、DevOps チームが以下のようなメリットを享受できるようにします :

* コンピューティングリソースの最適化(CPU、メモリなど)
* オートスケーリング
* CI/CD パイプラインの簡素化

=== このラボのゴール

目標は、*Red Hat Runtimes* 上でサーバーレスアプリケーションを開発し、 https://www.openshift.com/learn/topics/serverless[OpenShift Serverless^] を使用して *OpenShift 4* 上にデプロイし、Cloud Native で継続的なインテグレーションとデリバリ(CI/CD)パイプラインを使用することです。このラボでは、Knative Serving、Istio、Tekelton Pipelines を使用して、Quarkus ベースのサーバーレスアプリケーションとして Payment Service をデプロイします。このラボの後には、次のようなものを完成させる必要があります。

image::lab3-goal.png[goal, 700]

Knative Kafka Event _source_ は、Apache Kafka との _Knative Eventing_ の統合を可能にします。Apache Kafka でメッセージが生成されると、Apache Kafka イベントソースは生成されたメッセージを消費し、対応する Event _sink_ にそのメッセージを投稿します。

==== Red Hat OpenShift Serverless とは?

OpenShift Serverless は、開発者がオンデマンドでスケールアップまたはゼロにスケールアップするアプリケーションのデプロイと実行を支援します。アプリケーションは OCI に準拠した Linux コンテナとしてパッケージ化されており、どこでも実行可能です。

image::knative-serving-diagram.png[knative, 800]

アプリケーションは、自社のアプリケーションからのイベント、複数のプロバイダーからのクラウドサービス、Software as a Service(SaaS)システム、Red Hat Services( https://access.redhat.com/products/red-hat-amq[AMQ Streams^] )など、さまざまなイベントソースからトリガーすることができます。

image::knative-eventing-diagram.png[knative, 800]

OpenShift Serverless アプリケーションは、OpenShift https://www.openshift.com/learn/topics/pipelines[Pipelines^]、 https://www.openshift.com/learn/topics/service-mesh[Service Mesh^] 、Monitoring、 https://github.com/operator-framework/operator-metering[Metering^] などの他の OpenShift サービスと統合することができ、完全なサーバーレスアプリケーションの開発とデプロイメントの経験を提供します。

=== 1. ネイティブ実行可能なファイルのビルド

それでは、例の Quarkus アプリケーション用のネイティブ実行ファイルを作成してみましょう。これにより、アプリケーションの起動時間が改善され、最小限のディスクとメモリのフットプリントが生成されます。実行ファイルには、`JVM` (アプリケーションを実行するのに十分な大きさに縮小されています) とアプリケーションを含む、アプリケーションを実行するためのすべてが含まれています。これは https://graalvm.org/[GraalVM^] を使用して達成されます。

`GraalVM` は、JavaScript、Python、Ruby、R、Java、Scala、Groovy、Kotlin、Clojure などの JVM ベースの言語、C や C++ などの LLVM ベースの言語で書かれたアプリケーションをコンパイルして実行するための普遍的な仮想マシンです。先行コンパイル、積極的なデッドコードの排除、ネイティブバイナリとしての最適なパッケージングが含まれており、多くの起動ロジックをビルド時に移動させることで、起動時間とメモリリソース要件を大幅に削減します。

image::native-image-process.png[serverless, 700]

`GraalVM` はすでにインストールされています。CodeReady Workspaces ターミナルの `GRAALVM_HOME` 変数の値を確認してください :

[source,sh,role="copypaste"]
----
echo $GRAALVM_HOME
----

このステップでは、アプリケーションをネイティブ実行ファイルにコンパイルし、ローカルマシン上でネイティブイメージを実行する方法を学びます。

ネイティブイメージのコンパイルは、通常の JAR ファイル (バイトコード) のコンパイルよりも時間がかかります。しかし、このコンパイル時間は、アプリケーションが起動するたびに発生するのではなく、一度だけ発生します。

Quarkusが _SuperSonic Subatomic Subatomic Java_ と名乗る理由を調べてみましょう。サンプルアプリを作ってみましょう。CodeReady Workspace ターミナルで、以下のコマンドを実行します:

[source,sh,role="copypaste"]
----
mkdir /tmp/hello && cd /tmp/hello && \
mvn io.quarkus:quarkus-maven-plugin:1.7.5.Final-redhat-00007:create \
    -DprojectGroupId=org.acme \
    -DprojectArtifactId=getting-started \
    -DplatformGroupId=com.redhat.quarkus \
    -DplatformVersion=1.7.5.Final-redhat-00007 \
    -DclassName="org.acme.quickstart.GreetingResource" \
    -Dpath="/hello"
----

これは、 */tmp/hello* ディレクトリにシンプルな Quarkus アプリを作成します。

次に、このコマンドで `ネイティブ実行ファイル` を作成します :

[source,sh,role="copypaste"]
----
mvn -f /tmp/hello/getting-started/pom.xml clean package -Pnative -DskipTests -Dquarkus.native.native-image-xmx=2g
----

これは実行に1～2分かかるかもしれません。Quarkus の利点の 1 つは、デッドコードやプロセスアノテーションなどを最適化して削除するためのビルド時間が長くなることが犠牲となっても、起動時間が驚くほど速いことです。これは、毎回の起動ではなく、ビルド時に一度だけ発生します。

[NOTE]
====
この環境ではLinuxを使用しており、最終的にアプリケーションを実行する OS も Linux なので、ローカル OS を使用してネイティブの Quarkus アプリをビルドすることができます。Windows や Mac OS X などの他の OS 上でネイティブの Linux バイナリをビルドする必要がある場合は、Docker をインストールして、`mvn clean package -Pnative -Dnative-image.docker-build=true -DskipTests=true` を使用する必要があります。
====

image::payment-native-image-build.png[serverless, 700]

ネイティブビルドの出力はネイティブLinuxバイナリです。 `readelf` コマンドを利用して確認できます。 ターミナルでこれを実行してください:

[source,sh,role="copypaste"]
----
readelf -h /tmp/hello/getting-started/target/*-runner
----

以下が確認できます。

[source,console]
----
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF64
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           Advanced Micro Devices X86-64
  Version:                           0x1
  ....
----

これはLinux上でのみ実行可能なバイナリです。しかし、すぐにご覧いただくことになりますが、このネイティブ実行ファイルは _非常に_ 高速に起動し、メモリ消費も少なくなっています。

ここでの環境は Linux なので、実行するだけで OK です。CodeReady Workspaces Terminal で実行します :

[source,sh,role="copypaste"]
----
/tmp/hello/getting-started/target/*-runner
----

[WARNING]
====
別の CodeReady Workspace Terminal で以前のアプリケーションをまだ実行している場合、`java.net.BindException. Address already in use` のようなエラーが出るかもしれません。別のタブに移動してkbd:[CTRL+C] を押して前のアプリケーションを停止してから、ネイティブアプリケーションを再度実行してみてください。
====

驚くほど速い起動時間に注目してください :

[source,shell]
----
__  ____  __  _____   ___  __ ____  ______ 
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
2020-10-22 01:57:51,424 INFO  [io.quarkus] (main) getting-started 1.0-SNAPSHOT native (powered by Quarkus x.x.x) started in 0.016s. Listening on: http://0.0.0.0:8080
2020-10-22 01:57:51,424 INFO  [io.quarkus] (main) Profile prod activated. 
2020-10-22 01:57:51,424 INFO  [io.quarkus] (main) Installed features: [cdi, resteasy]
----

That’s *16 milliseconds* to start up. The start-up time might be different in your environment.

また、Linux の `ps` ユーティリティで報告されているように、メモリ使用量が非常に少なくなっています。アプリを実行している間、別のターミナルで以下のコマンドを実行します :

[source,sh,role="copypaste"]
----
ps -o pid,rss,command -p $(pgrep -f runner)
----

以下のような出力を確認できます :

[source,shell]
----
    PID   RSS COMMAND
   4506 61024 /tmp/hello/getting-started/target/getting-started-1.0-SNAPSHOT-runner
----

This shows that our process is taking around 61MB of memory (https://en.wikipedia.org/wiki/Resident_set_size[Resident Set
Size^], or RSS). Pretty compact!

[NOTE]
====
Quarkus を含むあらゆるアプリの RSS やメモリ使用量は、特定の環境によって異なり、アプリの負荷を経験すると上昇します。
====

アプリが動作することを確認してください。新しい CodeReady Workspaces Terminal で実行します :

[source,sh,role="copypaste"]
----
curl -i http://localhost:8080/hello
----

以下の返却を確認することができます :

[source,console]
----
HTTP/1.1 200 OK
Content-Length: 5
Content-Type: text/plain;charset=UTF-8

hello
----

*おめでとうございます。* これで、Javaアプリケーションをネイティブ実行可能な JAR と Linux ネイティブバイナリとして構築することができました。ネイティブバイナリの利点については、後ほど Kubernetes へのデプロイを開始したときに探ってみましょう。

実行中の Quarkus を kbd:[CTRL+C] で必ず終了させてください。

=== 2. 古い payment-service の削除

_OpenShift Serverless_ は Knative Serving をベースに構築されており、サーバーレスアプリケーションと機能のデプロイと提供をサポートします。サーバレスアプリケーションと機能のデプロイと提供をサポートするために、_Serverless_は簡単に始められ、高度なシナリオをサポートするために拡張できます。

OpenShift Serverless は、それを可能にするミドルウェアプリミティブを提供します :

* サーバーレスコンテナの高速デプロイメント
* 自動スケールアップとゼロへのスケールダウン
* Istio コンポーネントのためのルーティングとネットワークプログラミング
* 配置されたコードとコンフィギュレーションのポイントインタイムスナップショット

ラボでは、_OpenShift Serverless Operator_ はすでに OpenShift 4クラスタにインストールされていますが、あなた自身の OpenShift クラスタにインストールしたい場合は、 https://docs.openshift.com/container-platform/latest/serverless/installing-openshift-serverless.html[Installing OpenShift Serverless^] に従ってください。

まず、既存の `BuildConfig` は前回のラボでデプロイした拡張可能な Jar をベースにしているので削除する必要があります。

[source,sh,role="copypaste"]
----
oc delete bc/payment imagestream.image.openshift.io/payment
----

また、必要に応じて支払いサービスのデプロイと管理ポッドへのトラフィックのルーティングを Knative が処理するため、既存の支払いの _Deployment_ と _Route_ も削除します :

[source,sh,role="copypaste"]
----
oc delete dc/payment route/payment svc/payment
----

=== 3. Apache Kafka を使用した Knative Eventing Integration の有効化

_Knative Eventing_ は、Cloud Native 開発における共通のニーズに応えるために設計されたシステムであり、以下の目標を達成するために、イベントソースとイベントコンシューマーを `late-binding` することを可能にするための構成可能なプリミティブを提供します :

* サービスは開発中に緩く結合され、独立してデプロイされます。
* プロデューサーは、コンシューマーがリッスンする前にイベントを生成することができ、コンシューマーはまだ生成されていないイベントやイベントのクラスに興味を示すことができます。
* プロデューサやコンシューマを変更することなく、特定のプロデューサからイベントの特定のサブセットを選択する機能を備えたサービスを接続して、新しいアプリケーションを作成することができます。

プロデューサやコンシューマを変更することなく、特定のプロデューサからイベントの特定のサブセットを選択する機能を備えたサービスを接続して、新しいアプリケーションを作成することができます。

Knative の直接の統合コードを削除します。現在、私たちの支払いサービスは、Kafka に直接バインドしてイベントをリッスンしています。Knative のイベント統合ができたので、このコードは不要になりました。payment-service `/src/main/java/com/redhat/cloudnative` ディレクトリにある `PaymentResource.java` ファイルを開きます。

kbd:[CTRL+/] (Mac OS の場合は kbd:[Command+/] ) で `onMessage()` メソッドをコメントアウトしてください :

[source,java]
----
//    @Incoming("orders")
//    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
//            throws IOException {
//
//        log.info("Kafka message with value = {} arrived", message.getPayload());
//        handleCloudEvent(message.getPayload());
//        return message.ack();
//    }
----

また、着信ストリームの設定を削除します。application.propertiesで、_Incoming_ ストリームと _OpenShift extension_ について、以下の行を kbd:[CTRL+/](Mac OSの場合は kbd:[Command+/]) でコメントアウトしてください。

[source,none]
----
# OpenShift extension
# quarkus.kubernetes-client.trust-certs=true
# quarkus.container-image.build=true
# quarkus.kubernetes.deploy=true
# quarkus.kubernetes.deployment-target=openshift
# quarkus.openshift.expose=true
# quarkus.openshift.labels.app.openshift.io/runtime=quarkus
# quarkus.s2i.base-jvm-image=registry.access.redhat.com/ubi8/openjdk-11

...

# Incoming stream (unneeded when using Knative events)
# mp.messaging.incoming.orders.connector=smallrye-kafka
# mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
# mp.messaging.incoming.orders.group.id=payment-order-service
# mp.messaging.incoming.orders.auto.offset.reset=earliest
# mp.messaging.incoming.orders.enable.auto.commit=true
# mp.messaging.incoming.orders.request.timeout.ms=30000
----

[WARNING]
====
`mp.messaging.incoming`, `OpenShift extension` で始まる行だけをコメントアウトするか削除して、残りを残してください!
====

Then append the following configuration for native compilation using `Mandrel` builder image:

[source,properties,role="copypaste"]
----
quarkus.container-image.group={{ USER_ID }}-cloudnativeapps// <1>
quarkus.container-image.registry=image-registry.openshift-image-registry.svc:5000
quarkus.kubernetes-client.trust-certs=true
quarkus.kubernetes.deployment-target=knative// <2>
quarkus.kubernetes.deploy=true
quarkus.native.native-image-xmx=4g// <3>
quarkus.openshift.expose=true
----

<1> Define a project name where you deploy a serverless application
<2> Enable the generation of Knative resources
<3> Mximum Java heap to be used during the native image generation

[NOTE]
====
Mandrel is a downstream distribution of the GraalVM community edition. Mandrel's main goal is to provide a native-image release specifically to support https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus[Quarkus^]. The aim is to align the native-image capabilities from GraalVM with OpenJDK and Red Hat Enterprise Linux libraries to improve maintainability for native Quarkus applications.
====

In order to complie a native binary, Let's remove a specified package type(i.e. uberJar) configuration in `pom.xml`. Open the _pom.xml_ then remove or comment the following configuration option:

image::remove-uberjar.png[serverless, 900]

Rebuild and re-deploy new payment service via running the following maven plugin in CodeReady Workspaces Terminal:

[source,sh,role="copypaste"]
----
mvn clean package -Pnative -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

The _-Pnative_ argument selects the native maven profile which invokes the _Graal compiler_. It will take a few minutes(up to 7 mins) to complete a native binary build as well as deploying a new knative service. After successful creation of the service we should see a *Knative Service*(_KSVC_) and *Revision*(_REV_) in the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View]:

[NOTE]
====
It will take a few moments (up to 30 seconds) to fully render while the networking is setup properly. Try reloading the browser page if all you see is an empty box!
====

ラベルを編集して、_Quarkus_ アイコンを追加してみましょう。これまでは `oc label` コマンドで行っていましたが、今回は手動で行ってみましょう。支払い *REV* をクリックして、_Actions_ ドロップボックスで *Edit Labels* を選択します :

image::kservice-up.png[serverless, 700]

このラベルを追加し、*Save*をクリックします :

[source,sh,role="copypaste"]
----
app.openshift.io/runtime=quarkus
----

image::quarkus-label.png[serverless, 500]

これでトポロジー上の _Payment Service_ に Quarkus のアイコンが表示されるようになりました :

image::kservice-up-quarkus.png[serverless, 700]

ラボ環境では、_OpenShift Serverless_ は、サービス(つまり支払い)は *30秒間* リクエストがない場合に自動的にサービスをゼロにスケールダウンしますこれは、支払いサービスの Pod が 30 秒後に利用できなくなることを意味します。もう一度 {{CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] にアクセスしてください。決済サービスに *青丸* がないことを確認してください!

[NOTE]
====
0 にスケールする前に 〜30秒 待つ必要があります!
====

image::kservice-down.png[serverless, 700]

このエンドポイントにトラフィックを送信すると、アプリをスケールアップするためのオートスケーラーがトリガーされます。 http://payment-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN }}[Open URL^] をクリックして、支払いサービスを _trigger_ します。これにより、いくつかのダミーデータが `payment` サービスに送信されますが、より重要なのは、knative が自動的にポッドを再びスピンアップさせ、30秒後にシャットダウンするようにトリガーしたことです。

image::payment-serving-magic.png[serverless, 700]

*おめでとうございます！* これで、支払いサービスが Quarkus ネイティブイメージとしてデプロイされ、_OpenShift Serverless_ で提供され、従来のJavaアプリケーションよりも高速になりました。これでServerlessの能力は終わりではありませんので、次の演習で支払いサービスがどのように _魔法のように_ スケールアップするかを見てみましょう。

それでは、*KafkaSource* を作成して *Knative Eventing* を有効にしてみましょう。このラボでは、_Knative Eventing_ はすでに OpenShift 4 クラスタの _Knative Eventing Operator_ を介してインストールされています。

{{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] に戻って、右上の `+` アイコンをクリックします。

image::plus-icon.png[serverless, 500]

以下の `KafkaSource` を `YAML` エディタでコピーし、*Create* をクリックする :

[source,yaml,role="copypaste"]
----
apiVersion: sources.knative.dev/v1beta1
kind: KafkaSource
metadata:
  name: kafka-source
spec:
  consumerGroup: knative-group
  bootstrapServers:
  - my-cluster-kafka-bootstrap.{{ USER_ID }}-cloudnativeapps:9092
  topics:
  - orders
  sink:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: payment
----

Kafka と *payments* サービスの間の新しい接続を見ることができます :

[NOTE]
====
Serverless は進化し続けている機能であり、この場合、使用している OpenShift や OpenShift serverless のバージョンによっては、トポロジービューに `KafkaSource` が表示されない場合があります。表示されない場合は心配しないでください。基礎となる技術は期待通りに動作しますので、先に進むだけです。
====

image::kafka-event-source-link.png[serverless, 700]

Coolstore Web UI経由で _Serverless_ の機能で決済サービスが正常に動作するかどうかを確認してみましょう。

=== 4. End to End Functional Testing

始める前に、 {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] で、_payment service_ が再び_ゼロ_にスケールダウンされているかどうかを確認する必要があります :

image::payment-down-again.png[serverless, 700]

ショッピングをしましょう! http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^] にアクセスしてください!

以下のショッピングシナリオでは、ショッピングカートにいくつかのクールなアイテムを追加します。:

[arabic]
. *Add to Cart* をクリックして、 _Pronounced Kubernetes_ をカートに追加してください。トップメニューの下に `Success! Added!` というメッセージが表示されます。

image::add-to-cart-serverless.png[serverless, 1000]

[arabic, start=2]
. *Cart* タブに移動し、 *Checkout* ボタンをクリックします。クレジットカード情報を入力します。カード情報は16桁で、数字の `4` で始まる必要があります。例えば、 `4123987754646678` のように入力します。 

image::checkout-serverless.png[serverless, 1000]

[arabic, start=3]
. クレジットカード情報を入力して、商品代金をお支払いください :

image::input-cc-info-serverless.png[serverless, 1000]

[arabic, start=4]
. _Kafka Event_ がどのようにして _Knative Eventing_ を有効にしているのか確認してみましょう。 {{CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] に戻って、*支払いサービス* が自動的に起動しているかどうか確認してください。

image::payment-serving-magic.png[serverless, 500]

[arabic, start=5]
. Confirm the _Payment Status_ of the your shopping items in the *All Orders* tab. It should be `Processing`.

image::payment-processing-serverless.png[serverless, 1000]


[arabic, start=5]
. しばらく待ってから *Orders* ページをリロードして、支払い状況が `COMPLETED` か `FAILED` になっていることを確認してください。

[NOTE]
====
ステータスが *Processing* のままの場合は、注文サービスが受信した Kafka メッセージを処理して MongoDB に保存しています。あと数回ページをリロードしてください。
====

image::payment-completedorfailed-serverless.png[serverless, 1000]

これはこれまでと同じ結果ですが、Knative のイベントを使って、需要に合わせてスケールできる、より強力なイベント駆動型のシステムを作ることができます。

=== 5. Tekton を使用した Cloud-Native CI/CD パイプラインの作成

Cloud Native アプリケーション/マイクロサービスを構築、テスト、デプロイ、管理するためのオープンソース CI/CD ツールは、オンプレミスからプライベート、パブリック、ハイブリッドクラウドまで、数多く存在します。各ツールは、既存のプラットフォーム/システムと統合するためのさまざまな機能を提供しています。そのため、DevOps チームが CI/CD パイプラインを作成し、Kubernetes クラスター上で保守することができなくなることがあります。 *Cloud Native CI/CD パイプライン* は、Kubernetes Native の方法で定義し、実行する必要があります。例えば、パイプラインは YAML 形式で Kubernetes リソースとして指定することができます。

*OpenShift Pipelines* は、Tektonを使用してパイプラインを構築するためのクラウドネイティブの継続的インテグレーション＆デリバリ（CI/CD）ソリューションです。Tektonは柔軟性に優れたKubernetesネイティブのオープンソースCI/CDフレームワークであり、以下のような特徴をによって足回りの詳細を抽象化することで、複数のプラットフォーム（Kubernetes、サーバーレス、VMなど）にまたがるデプロイメントを自動化することができます:

* Tektonに基づく標準ベースのCI/CDパイプライン定義
* S2I、Buildah、Buildpacks、KanikoなどのKubernetesツールを使ってイメージを構築
* Kubernetes、サーバーレス、VMなどの複数のプラットフォームへのアプリケーションのデプロイ
* 既存のツールとの拡張・統合が容易
* パイプラインをオンデマンドで拡張
* あらゆるKubernetesプラットフォームに対応した移植性
* マイクロサービスや分散型チームのための設計
* OpenShift Developer Consoleとの統合

[NOTE]
OpenShift Pipelines プロジェクト は Developer Preview リリースです。Developer Preview リリースには、完全にテストされていない可能性のある機能が含まれています。お客様は、このリリースを使用してフィードバックを提供することをお勧めします。Red Hat は報告された問題を修正することを約束しておらず、提供された機能は将来のリリースで利用できない可能性があります。

ラボでは OpenShift 4 クラスタに OpenShift Pipelines が既にインストールされていますが、自分の OpenShift クラスタに OpenShift Pipelines をインストールしたい場合は、OpenShift OperatorHub で利用できるオペレーターを介してインストールできる OpenShift 上のアドオンとして OpenShift Pipelines が提供されています。

パイプラインを定義するには、以下のように _カスタムリソース_ を作成する必要があります。:

* *Task*: 特定のタスクを実行する再利用可能な疎結合のステップ数(例：コンテナイメージの構築)。
* *Pipeline*: パイプラインの定義と実行すべきタスク
* *PipelineResource*: パイプラインやタスクへの入力 (git リポジトリなど) と出力 (イメージレジストリなど) の出し入れ。
* *TaskRun*: タスクのインスタンスの実行結果 (例 成功 or 失敗)
* *PipelineRun*: パイプラインの実行結果 (例 成功 or 失敗)

image::tekton-arch.png[severless, 800]

パイプラインの概念の詳細については、パイプラインの定義に使用できる様々なパラメータや属性を理解するための優れたガイドを提供している https://github.com/tektoncd/pipeline/tree/master/docs#learn-more[Tekton documentation^] を参照してください。

Tekton APIは、ステップを再利用できるようにコンフィギュレーションから機能を分離することができます（Pipelines vs PipelineRunsなど）が、これらのコンフィギュレーションを動的にカプセル化するリソース（特にPipelineRunsやPipelineResources）を生成するメカニズムは提供されていません。 Triggersは、以下のCRDでTektonアーキテクチャを拡張しています:

* *TriggerTemplate* - 作成するリソースをテンプレート化します（例： PipelineResourcesと、それを使用するPipelineRunの作成）。
* *TriggerBinding* - イベントを検証し、ペイロードフィールドを抽出します。
* *EventListener* - TriggerBinding と TriggerTemplate をアドレス指定可能なエンドポイント（イベントシンク）に接続します。各 TriggerBinding から抽出されたイベントパラメータ（および供給された静的パラメータ）を使用して、対応する TriggerTemplate で指定されたリソースを作成します。また、外部サービスがインターセプターフィールドを介してイベントペイロードを前処理することも可能です。
* *ClusterTriggerBinding* - クラスタスコープ付き TriggerBinding

tektoncd/pipeline と一緒に _tektoncd/triggers_ を使うことで、実行がすべてKubernetesリソースで定義されている本格的なCI/CDシステムを簡単に作ることができます。

このラボでは、パイプラインの概念と、OpenShift Serverless プラットフォーム上でマイクロサービスを構築してデプロイするための CI/CD パイプラインの作成と実行方法について説明します。

https://github.com/openshift-pipelines/vote-ui[frontend^] と https://github.com/openshift-pipelines/vote-api[backend^] を `{{ USER_ID }}-cloudnative-pipeline` プロジェクトにデプロイしてみましょう。

*Task* は、順次実行される複数のステップで構成されています。TaskRun を作成することで _Task_ が実行されます。TaskRun は1つの Pod をスケジュールします。各ステップは、同じポッド内の別のコンテナで実行されます。また、パイプライン内の他のタスクと相互作用するために、入力と出力を持つこともできます。

_task_ が実行を開始すると、ポッドを起動し、同じポッド上の別のコンテナ内で各ステップを順次実行します。このタスクはたまたま単一のステップを持っていますが、タスクは複数のステップを持つことができ、同じポッド内で実行しているので、ファイルのキャッシュや configmaps、secret などにアクセスするために同じボリュームにアクセスすることができます。タスク `Tasks` は入力 (git リポジトリなど) と出力 (レジストリのイメージなど) を受け取って相互にやりとりすることもできます。
_task_ が実行を開始すると、ポッドを起動し、同じポッド上の別のコンテナ内で各ステップを順次実行します。このタスクはたまたま1つのステップを持っていますが、タスクは複数のステップを持つことができ、同じポッド内で実行されるため、同じボリュームにアクセスすることができます。そのため、ファイルのキャッシュやconfigmaps、secretなどにアクセスすることが可能です。ワークスペースを使ってボリュームを指定することもできます。Taskは書き込み可能なワークスペースを最大1つ使用することをお勧めします。ワークスペースにはsecret、pvc、config、emptyDirを指定できます。

[NOTE]
====
git リポジトリの要件だけがタスクに宣言されていて、使用する特定の git リポジトリを指定しているわけではありません。これにより、_tasks_ を複数のパイプラインや目的に合わせて再利用できるようになります。再利用可能な _task_ の例は https://github.com/tektoncd/catalog[Tekton Catalog^] や https://github.com/openshift/pipelines-catalog[OpenShift Catalog^] リポジトリにあります。
====

Java アプリケーションをコンパイルしてデプロイする方法を知っている、パイプラインで使用する2つの定義済みの Tekton タスクを作成する必要があります。以下のコマンドを使って `apply-manifests` と `update-deployment` Tekton タスクをインストールします :

[source,sh,role="copypaste"]
----
oc project {{ USER_ID }}-cloudnative-pipeline &&
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/apply_manifests_task.yaml && oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/update_deployment_task.yaml
----

すでに CodeReady Workspaces にインストールされている https://github.com/tektoncd/cli/releases[Tekton CLI^] を使って、*tasks* が正しくインストールされているか確認してみましょう :

[source,sh,role="copypaste"]
----
tkn task list
----

以下の二つのタスクを確認できます :

[source,sh]
----
NAME               AGE
apply-manifests   10 seconds ago
update-deployment 10 seconds ago

ここでは https://buildah.io/[buildah^] clusterTasks を使用します。これは Operator と一緒にインストールされます。Operatorは、ご覧のようにいくつかのClusterTaskをインストールします:

[source,sh,role="copypaste"]
----
tkn clustertasks ls | grep buildah
----

以下のようなClusterTaskが表示されます:

[source,sh]
----
buildah                    Buildah task builds...   5 hours ago
buildah-pr                 Buildah task builds...   5 hours ago
buildah-pr-v0-14-3         Buildah task builds...   5 hours ago
buildah-v0-14-3            Buildah task builds...   5 hours ago

パイプラインは、実行されるべきタスクの数と、それらの入力と出力を介した相互作用の方法を定義します。

このラボでは、GitHubからアプリケーションのソースコードを取り出し、OpenShift上にビルドしてデプロイするパイプラインを作成します。

image::pipeline-diagram.png[serverless, 800]

このパイプラインは、設定されたリソースを使用してバックエンド/フロントエンドをビルドしてデプロイするのに役立ちます。ここでは、パイプラインの高レベルな説明をします:

<1> (_git-url_ と _git-revision_ パラメーター) を参照して git リポジトリからアプリケーションのソースコードをクローンします。
<2> Buildahを使用してアプリケーションのコンテナイメージをビルドする_buildah_クラスタタスクを使用してアプリケーションのコンテナイメージをビルドします。
<3> (_image_ param) を参照して、アプリケーションの画像を画像レジストリにプッシュします。
<4> 新しいアプリケーションイメージは、 _apply-manifests_ と _update-deployment_ タスクを使用してOpenShift上にデプロイされます。

[NOTE]
====
お気づきのように、パイプラインでプッシュされるgitリポジトリやイメージレジストリへの参照がないことに気づかれたかもしれません。これは、Tekton のパイプラインは、アプリケーションのライフサイクルを通して、環境やステージを越えて再利用できるように設計されているからです。
====

パイプラインは、生成される git ソースリポジトリやイメージの詳細を抽象化して _PipelineResources_ や _Params_ とします。パイプラインをトリガーする際には、パイプラインの実行時に使用する git リポジトリやイメージのレジストリを指定することができます。もうちょっと我慢してください!これについては次のセクションで少し説明します。

_tasks_ の実行順序は、*inputs* と *outputs* で定義されたタスク間の依存関係と、*runAfter* で定義された明示的な順序によって決定されます。

_workspaces_ フィールドでは、パイプライン内の各タスクが実行中に必要とする1つまたは複数のボリュームを指定できます。ワークスペースは、 _workspaces_ フィールドで1つ以上指定します。

OpenShiftコンソールでは、まず _Topology_ ビューにいることを確認してから、ドロップダウンを使用して `{{USER_ID}}}-cloudnative-pipeline` プロジェクトを選択します:

image::pipeline-project-select.png[serverless, 800]

次に、右上の `+` ボタンをクリックし、以下のYAMLを貼り付けて *Create* をクリックします:

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: build-and-deploy
spec:
  workspaces:
  - name: shared-workspace
  params:
  - name: deployment-name
    type: string
    description: name of the deployment to be patched
  - name: git-url
    type: string
    description: url of the git repo for the code of deployment
  - name: git-revision
    type: string
    description: revision to be used from repo of the code for deployment
    default: "master"
  - name: IMAGE
    type: string
    description: image to be build from the code
  tasks:
  - name: fetch-repository
    taskRef:
      name: git-clone
      kind: ClusterTask
    workspaces:
    - name: output
      workspace: shared-workspace
    params:
    - name: url
      value: $(params.git-url)
    - name: subdirectory
      value: ""
    - name: deleteExisting
      value: "true"
    - name: revision
      value: $(params.git-revision)
  - name: build-image
    taskRef:
      name: buildah
      kind: ClusterTask
    params:
    - name: TLSVERIFY
      value: "false"
    - name: IMAGE
      value: $(params.IMAGE)
    workspaces:
    - name: source
      workspace: shared-workspace
    runAfter:
    - fetch-repository
  - name: apply-manifests
    taskRef:
      name: apply-manifests
    workspaces:
    - name: source
      workspace: shared-workspace
    runAfter:
    - build-image
  - name: update-deployment
    taskRef:
      name: update-deployment
    params:
    - name: deployment
      value: $(params.deployment-name)
    - name: IMAGE
      value: $(params.IMAGE)
    runAfter:
    - apply-manifests
----

作成したパイプラインが表示されます :

image::console-import-yaml-2.png[serverless, 800]

パイプラインが作成されたので、CodeReady Workspaces Terminalの _Tekton CLI_ を使って作成したパイプラインのリストを確認します:

[source,sh,role="copypaste"]
----
tkn pipeline ls
----

作成されたリソースの一覧を見ることができます:

[source,shell]
----
NAME               AGE              LAST RUN   STARTED   DURATION   STATUS
build-and-deploy   12 minutes ago   ---        ---       ---        ---
----

パイプライン実行に使用できる *PersistentVolumeClaim* を作成する必要があります:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/persistent_volume_claim.yaml
----

A *PipelineRun* is how you can start a pipeline and tie it to the Git and image resources that should be used for this specific invocation. Let's go to {{ CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] and click on *Start*:
*PipelineRun* は、パイプラインを起動して、この特定の呼び出しに使用するべきGitやイメージリソースに結びつける方法です。では、 {{ CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] に移動して、 *Start* をクリックしてみましょう:

image::pipeline-start.png[serverless, 800]

このダイアログボックスでは、 _build_ ステップのソースリポジトリの最終ターゲット値と、 _deploy_ ステップでデプロイするイメージの名前をバインドします。パラメータを入力して、以下のようにリソースを選択し、 *Start* をクリックします:

* deployment-name: `vote-api`
 * git-url: `http://github.com/openshift-pipelines/vote-api.git`
 * git-revision: `master`
 * IMAGE: `image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/vote-api`
 * shared-workspace: `source-pvc` in *PVC*

image::pipeline-start-popup.png[serverless, 700]

パイプラインの *build-and-deploy* を開始するとすぐにpipelinerunがインスタンス化され、パイプラインで定義されているタスクを実行するためのポッドが作成されます。数分後、パイプラインは正常に終了するはずです。ステップの上にカーソルを置くと、ステップの進捗状況を簡単にスナップショットで見ることができます。

image::pipeline-complete.png[serverless, 800]

それでは、 `vote-ui` アプリケーションをデプロイするためのパイプラインを実行してみましょう。 {{CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] に戻って、 *Start* をクリックします。次にパラメータを入力し、以下のようにリソースを選択して、 *Start* をクリックします:

 * deployment-name: `vote-ui`
 * git-url: `http://github.com/openshift-pipelines/vote-ui.git`
 * git-revision: `master`
 * IMAGE: `image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/vote-ui`
 * shared-workspace: `source-pvc` in *PVC*

image::pipeline-start-popup2.png[serverless, 700]

ビルドが完了したら、 {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}/cloudnative-pipeline[Topology View^] 上で、 *Open URL* をクリックします。Vote UIが正常にビルドされ、デプロイされていることが確認できるはずです。

image::voteui-openurl.png[serverless, 800]

=== 概要

このモジュールでは、様々なビジネスユースケースを処理するためのクラウドネイティブアプリケーションの開発方法を学びました。具体的には、複数のJavaランタイム（QuarkusとSpring Boot）、Javascript（Node.js）、異なるデータソース（PostgreSQL、MongoDBなど）を使用して、REST APIを使用したリアルタイムの _request/response_ 通信、 _Red Hat Data Grid_ を使用した高パフォーマンスのキャッシュ可能なサービス、 _Red Hat AMQ Streams_ でApache Kafkaを使用したイベントドリブン/リアクティブなショッピングカートサービスを実装してきました。

次に、 _OpenShift Serverless_ とKnativeを使って決済サービスを *Serverless* アプリケーションに変換しました。

最後に、OpenShift Pipelinesを使って再利用可能なCI/CDパイプラインを作成しました。

*Red Hat Runtimes* は、企業の開発者が高度なクラウドネイティブアーキテクチャを設計し、*Red Hat OpenShift Container Platform* 上のハイブリッドクラウド上でクラウドネイティブアプリケーションの開発、構築、デプロイを行うことを可能にします。おめでとうございます！

==== Additional Resources:

* https://learn.openshift.com/developing-with-quarkus/[Quarkus Tutorials Right in Your Browser^]
* https://developers.redhat.com/articles/quarkus-quick-start-guide-kubernetes-native-java-stack/[Quarkus Quickstart Guide]
* https://docs.openshift.com/container-platform/latest/serverless/serverless-getting-started.html[Getting started with OpenShift Serverless^]
* https://www.openshift.com/learn/topics/pipelines[Cloud-native CI/CD on OpenShift^]
* https://developers.redhat.com/topics/serverless-architecture/[Serverless Architecture Articles^]
