= Lab3 - サーバーレスでサービスを進化
:experimental:

Cloud Native アプリケーション・アーキテクチャでは、現在、複数のマイクロサービスを *reactive* システムで運用しています。しかし、アプリケーションやサービスが24時間稼働している必要はありません。何かがサービスを利用する必要があるときに、*オンデマンド* で動作していればいいのです。これが *serverless* アーキテクチャが人気を集めている理由の一つです。

* *Serverless* はしばしば _FaaS_ (Functions-as-a-Service)という用語と互換的に使われます。しかし、サーバーレスはサーバーがないという意味ではありません。実際には、サーバーは存在します - パブリッククラウドプロバイダーは、アプリケーションを展開、実行、管理するサーバーを提供します。
* *Serverless computing* は開発者がソフトウェアシステムを構築して提供する方法に変化をもたらす新興のカテゴリーです。アプリケーション・インフラストラクチャをコードから切り離すことで、開発プロセスを大幅に簡素化しながら、新たなコストと効率性のメリットをもたらすことができます。サーバーレスコンピューティングと FaaS は、Cloud Native サービスや https://enterprisersproject.com/hybrid-cloud[ハイブリッドクラウド^] とともに、エンタープライズ IT の次の時代を定義する上で重要な役割を果たすことになるでしょう。
* *Serverless platforms* は API を提供しており、ユーザーがコードスニペット (_actions_ とも呼ばれる関数) を実行し、各関数の結果を返すことができます。サーバレスプラットフォームは、開発者が関数の結果を取得するためのエンドポイントも提供しています。これらのエンドポイントは、他の関数の入力として使用することができ、それによって関連する関数のシーケンス(またはチェーン)を提供することができます。

サーバーレスアプリケーションは、DevOps チームが以下のようなメリットを享受できるようにします :

* コンピューティングリソースの最適化(CPU、メモリなど)
* オートスケーリング
* CI/CD パイプラインの簡素化

=== このラボのゴール

目標は、*Red Hat Runtimes* 上でサーバーレスアプリケーションを開発し、 https://www.openshift.com/learn/topics/serverless[OpenShift Serverless^] を使用して *OpenShift 4* 上にデプロイし、Cloud Native で継続的なインテグレーションとデリバリ(CI/CD)パイプラインを使用することです。このラボでは、Knative Serving、Istio、Tekelton Pipelines を使用して、Quarkus ベースのサーバーレスアプリケーションとして Payment Service をデプロイします。このラボの後には、次のようなものを完成させる必要があります。

image::lab3-goal.png[goal, 700]

Knative Kafka Event _source_ は、Apache Kafka との _Knative Eventing_ の統合を可能にします。Apache Kafka でメッセージが生成されると、Apache Kafka イベントソースは生成されたメッセージを消費し、対応する Event _sink_ にそのメッセージを投稿します。

==== Red Hat OpenShift Serverless とは?

OpenShift Serverless は、開発者がオンデマンドでスケールアップまたはゼロにスケールアップするアプリケーションのデプロイと実行を支援します。アプリケーションは OCI に準拠した Linux コンテナとしてパッケージ化されており、どこでも実行可能です。

image::knative-serving-diagram.png[knative, 800]

アプリケーションは、自社のアプリケーションからのイベント、複数のプロバイダーからのクラウドサービス、Software as a Service(SaaS)システム、Red Hat Services( https://access.redhat.com/products/red-hat-amq[AMQ Streams^] )など、さまざまなイベントソースからトリガーすることができます。

image::knative-eventing-diagram.png[knative, 800]

OpenShift Serverless アプリケーションは、OpenShift https://www.openshift.com/learn/topics/pipelines[Pipelines^]、 https://www.openshift.com/learn/topics/service-mesh[Service Mesh^] 、Monitoring、 https://github.com/operator-framework/operator-metering[Metering^] などの他の OpenShift サービスと統合することができ、完全なサーバーレスアプリケーションの開発とデプロイメントの経験を提供します。

=== 1. ネイティブ実行可能なファイルのビルド

それでは、例の Quarkus アプリケーション用のネイティブ実行ファイルを作成してみましょう。これにより、アプリケーションの起動時間が改善され、最小限のディスクとメモリのフットプリントが生成されます。実行ファイルには、`JVM` (アプリケーションを実行するのに十分な大きさに縮小されています) とアプリケーションを含む、アプリケーションを実行するためのすべてが含まれています。これは https://graalvm.org/[GraalVM^] を使用して達成されます。

`GraalVM` は、JavaScript、Python、Ruby、R、Java、Scala、Groovy、Kotlin、Clojure などの JVM ベースの言語、C や C++ などの LLVM ベースの言語で書かれたアプリケーションをコンパイルして実行するための普遍的な仮想マシンです。先行コンパイル、積極的なデッドコードの排除、ネイティブバイナリとしての最適なパッケージングが含まれており、多くの起動ロジックをビルド時に移動させることで、起動時間とメモリリソース要件を大幅に削減します。

image::native-image-process.png[serverless, 700]

`GraalVM` はすでにインストールされています。CodeReady Workspaces ターミナルの `GRAALVM_HOME` 変数の値を確認してください :

[source,sh,role="copypaste"]
----
echo $GRAALVM_HOME
----

このステップでは、アプリケーションをネイティブ実行ファイルにコンパイルし、ローカルマシン上でネイティブイメージを実行する方法を学びます。

ネイティブイメージのコンパイルは、通常の JAR ファイル (バイトコード) のコンパイルよりも時間がかかります。しかし、このコンパイル時間は、アプリケーションが起動するたびに発生するのではなく、一度だけ発生します。

Quarkusが _SuperSonic Subatomic Subatomic Java_ と名乗る理由を調べてみましょう。サンプルアプリを作ってみましょう。CodeReady Workspace ターミナルで、以下のコマンドを実行します:

[source,sh,role="copypaste"]
----
mkdir /tmp/hello && cd /tmp/hello && \
mvn io.quarkus:quarkus-maven-plugin:1.3.4.Final-redhat-00001:create \
    -DprojectGroupId=org.acme \
    -DprojectArtifactId=getting-started \
    -DplatformGroupId=com.redhat.quarkus \
    -DplatformVersion=1.3.4.Final-redhat-00001 \
    -DclassName="org.acme.quickstart.GreetingResource" \
    -Dpath="/hello"
----

これは、 */tmp/hello* ディレクトリにシンプルな Quarkus アプリを作成します。

次に、このコマンドで `ネイティブ実行ファイル` を作成します :

[source,sh,role="copypaste"]
----
mvn -f /tmp/hello/getting-started/pom.xml clean package -Pnative -DskipTests
----

これは実行に1～2分かかるかもしれません。Quarkus の利点の 1 つは、デッドコードやプロセスアノテーションなどを最適化して削除するためのビルド時間が長くなることが犠牲となっても、起動時間が驚くほど速いことです。これは、毎回の起動ではなく、ビルド時に一度だけ発生します。

[NOTE]
====
この環境ではLinuxを使用しており、最終的にアプリケーションを実行する OS も Linux なので、ローカル OS を使用してネイティブの Quarkus アプリをビルドすることができます。Windows や Mac OS X などの他の OS 上でネイティブの Linux バイナリをビルドする必要がある場合は、Docker をインストールして、`mvn clean package -Pnative -Dnative-image.docker-build=true -DskipTests=true` を使用する必要があります。
====

image::payment-native-image-build.png[serverless, 700]

ここでの環境は Linux なので、実行するだけで OK です。CodeReady Workspaces Terminal で実行します :

[source,sh,role="copypaste"]
----
/tmp/hello/getting-started/target/*-runner
----

[WARNING]
====
別の CodeReady Workspace Terminal で以前のアプリケーションをまだ実行している場合、`java.net.BindException. Address already in use` のようなエラーが出るかもしれません。別のタブに移動してkbd:[CTRL+C] を押して前のアプリケーションを停止してから、ネイティブアプリケーションを再度実行してみてください。
====

驚くほど速い起動時間に注目してください :

[source,shell]
----
__  ____  __  _____   ___  __ ____  ______
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/
2020-04-06 13:48:33,523 INFO  [io.quarkus] (main) getting-started 1.0-SNAPSHOT (powered by Quarkus xx.xx.xx) started in 0.018s. Listening on: http://0.0.0.0:8080
2020-04-06 13:48:33,523 INFO  [io.quarkus] (main) Profile prod activated.
2020-04-06 13:48:33,523 INFO  [io.quarkus] (main) Installed features: [cdi, resteasy]
----

起動に *18ミリ秒* かかります。お使いの環境によっては起動時間が異なる場合があります。

また、Linux の `ps` ユーティリティで報告されているように、メモリ使用量が非常に少なくなっています。アプリを実行している間、別のターミナルで以下のコマンドを実行します :

[source,sh,role="copypaste"]
----
ps -o pid,rss,command -p $(pgrep -f runner)
----

以下のような出力を確認できます :

[source,shell]
----
    PID   RSS COMMAND
   2438 27268 /tmp/hello/getting-started/target/getting-started-1.0-SNAPSHOT-runner
----

これは、私たちのプロセスが `27 MB` 程度のメモリを消費していることを示しています (https://en.wikipedia.org/wiki/Resident_set_size[Resident Set Size^] 、または RSS)。かなりコンパクトです。

[NOTE]
====
Quarkus を含むあらゆるアプリの RSS やメモリ使用量は、特定の環境によって異なり、アプリの負荷を経験すると上昇します。
====

アプリが動作することを確認してください。新しい CodeReady Workspaces Terminal で実行します :

[source,sh,role="copypaste"]
----
curl -i http://localhost:8080/hello
----

以下の返却を確認することができます :

[source,console]
----
HTTP/1.1 200 OK
Content-Length: 5
Content-Type: text/plain;charset=UTF-8

hello
----

*おめでとうございます。* これで、Javaアプリケーションをネイティブ実行可能な JAR と Linux ネイティブバイナリとして構築することができました。ネイティブバイナリの利点については、後ほど Kubernetes へのデプロイを開始したときに探ってみましょう。

実行中の Quarkus を kbd:[CTRL+C] で必ず終了させてください。

=== 2. 古い payment-service の削除

_OpenShift Serverless_ は Knative Serving をベースに構築されており、サーバーレスアプリケーションと機能のデプロイと提供をサポートします。サーバレスアプリケーションと機能のデプロイと提供をサポートするために、_Serverless_は簡単に始められ、高度なシナリオをサポートするために拡張できます。

OpenShift Serverless は、それを可能にするミドルウェアプリミティブを提供します :

* サーバーレスコンテナの高速デプロイメント
* 自動スケールアップとゼロへのスケールダウン
* Istio コンポーネントのためのルーティングとネットワークプログラミング
* 配置されたコードとコンフィギュレーションのポイントインタイムスナップショット

ラボでは、_OpenShift Serverless Operator_ はすでに OpenShift 4クラスタにインストールされていますが、あなた自身の OpenShift クラスタにインストールしたい場合は、 https://docs.openshift.com/container-platform/latest/serverless/installing-openshift-serverless.html[Installing OpenShift Serverless^] に従ってください。

まず、既存の `BuildConfig` は前回のラボでデプロイした拡張可能な Jar をベースにしているので削除する必要があります。

[source,sh,role="copypaste"]
----
oc delete bc/payment imagestream.image.openshift.io/payment
----

また、必要に応じて支払いサービスのデプロイと管理ポッドへのトラフィックのルーティングを Knative が処理するため、既存の支払いの _Deployment_ と _Route_ も削除します :

[source,sh,role="copypaste"]
----
oc delete dc/payment route/payment svc/payment
----

=== 3. Apache Kafka を使用した Knative Eventing Integration の有効化

_Knative Eventing_ は、Cloud Native 開発における共通のニーズに応えるために設計されたシステムであり、以下の目標を達成するために、イベントソースとイベントコンシューマーを `late-binding` することを可能にするための構成可能なプリミティブを提供します :

* サービスは開発中に緩く結合され、独立してデプロイされます。
* プロデューサーは、コンシューマーがリッスンする前にイベントを生成することができ、コンシューマーはまだ生成されていないイベントやイベントのクラスに興味を示すことができます。
* プロデューサやコンシューマを変更することなく、特定のプロデューサからイベントの特定のサブセットを選択する機能を備えたサービスを接続して、新しいアプリケーションを作成することができます。

プロデューサやコンシューマを変更することなく、特定のプロデューサからイベントの特定のサブセットを選択する機能を備えたサービスを接続して、新しいアプリケーションを作成することができます。

Knative の直接の統合コードを削除します。現在、私たちの支払いサービスは、Kafka に直接バインドしてイベントをリッスンしています。Knative のイベント統合ができたので、このコードは不要になりました。payment-service `/src/main/java/com/redhat/cloudnative` ディレクトリにある `PaymentResource.java` ファイルを開きます。

kbd:[CTRL+/] (Mac OS の場合は kbd:[Command+/] ) で `onMessage()` メソッドをコメントアウトしてください :

[source,java]
----
//    @Incoming("orders")
//    public CompletionStage<Void> onMessage(KafkaRecord<String, String> message)
//            throws IOException {
//
//        log.info("Kafka message with value = {} arrived", message.getPayload());
//        handleCloudEvent(message.getPayload());
//        return message.ack();
//    }
----

また、着信ストリームの設定を削除します。application.propertiesで、_Incoming_ ストリームと _OpenShift extension_ について、以下の行を kbd:[CTRL+/](Mac OSの場合は kbd:[Command+/]) でコメントアウトしてください。

[source,none]
----
# OpenShift extension
# quarkus.kubernetes-client.trust-certs=true
# quarkus.container-image.build=true
# quarkus.kubernetes.deploy=true
# quarkus.kubernetes.deployment-target=openshift
# quarkus.openshift.expose=true
# quarkus.openshift.labels.app.openshift.io/runtime=quarkus
# quarkus.s2i.base-jvm-image=registry.access.redhat.com/ubi8/openjdk-11

...

# Incoming stream (unneeded when using Knative events)
# mp.messaging.incoming.orders.connector=smallrye-kafka
# mp.messaging.incoming.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# mp.messaging.incoming.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# mp.messaging.incoming.orders.bootstrap.servers=my-cluster-kafka-bootstrap:9092
# mp.messaging.incoming.orders.group.id=payment-order-service
# mp.messaging.incoming.orders.auto.offset.reset=earliest
# mp.messaging.incoming.orders.enable.auto.commit=true
# mp.messaging.incoming.orders.request.timeout.ms=30000
----

[WARNING]
====
`mp.messaging.incoming`, `OpenShift extension` で始まる行だけをコメントアウトするか削除して、残りを残してください!
====

CodeReady Workspaces Terminal で 以下の maven プラグインを実行して、新しい決済サービスを再構築し、再デプロイします :

[source,sh,role="copypaste"]
----
mvn clean package -Pnative -DskipTests -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service
----

_-Pnative_ 引数は、_Graal コンパイラ_ を呼び出すネイティブの maven プロファイルを選択します。JAR ファイルを取得する古いビルド構成を削除しました。新しいネイティブにコンパイルされた Quarkus アプリを使用できる新しいビルド設定が必要です。このコマンドで新しいビルドコンフィグを作成します :

[source,sh,role="copypaste"]
----
oc new-build quay.io/quarkus/ubi-quarkus-native-binary-s2i:1.0 --binary --name=payment -l app=payment
----

Start and watch the build, which will take about 3-4 minutes to complete:

[source,sh,role="copypaste"]
----
oc start-build payment --from-file=$CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/target/payment-1.0-SNAPSHOT-runner --follow
----

このステップでは、ネイティブバイナリとベース OS イメージを組み合わせ、新しいコンテナイメージを作成し、内部イメージレジストリにプッシュします。

{{ CONSOLE_URL }}/add/ns/{{ USER_ID }}-cloudnativeapps[+Add] に移動して、_Knative Service_ を作成し、*Container Image* をクリックします。:

image::add-icon.png[serverless, 700]

次の変数を入力して、先ほど作成した _Payment Image_ をデプロイし、*Create* をクリックします :

* Image stream tag from internal registry: *Checked*
* Projects: *{{ USER_ID }}-cloudnativeapps*
* ImageStreams: *payment*
* Tag: *latest*

* Application: *Create Application*
* Application Name: *payment*
* Name: *payment*

* Knative Service: *Checked*

image::deploy-payment-image.png[serverless, 900]

サービスの作成に成功すると、 {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View] に *Knative Service* (_KSVC_)と *Revision* (_REV_)が表示されるはずです :

[NOTE]
====
ネットワークが適切に設定されている間は、完全にレンダリングされるまでに数分(最大2分)かかります。空のボックスが表示される場合は、ブラウザのページをリロードしてみてください。
====

ラベルを編集して、_Quarkus_ アイコンを追加してみましょう。これまでは `oc label` コマンドで行っていましたが、今回は手動で行ってみましょう。支払い *REV* をクリックして、_Actions_ ドロップボックスで *Edit Labels* を選択します :

image::kservice-up.png[serverless, 700]

このラベルを追加し、*Save*をクリックします :

[source,sh,role="copypaste"]
----
app.openshift.io/runtime=quarkus
----

image::quarkus-label.png[serverless, 500]

これでトポロジー上の _Payment Service_ に Quarkus のアイコンが表示されるようになりました :

image::kservice-up-quarkus.png[serverless, 700]

ラボ環境では、_OpenShift Serverless_ は、サービス(つまり支払い)は *30秒間* リクエストがない場合に自動的にサービスをゼロにスケールダウンしますこれは、支払いサービスの Pod が 30 秒後に利用できなくなることを意味します。もう一度 {{CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] にアクセスしてください。決済サービスに *青丸* がないことを確認してください!

[NOTE]
====
0 にスケールする前に 〜30秒 待つ必要があります!
====

image::kservice-down.png[serverless, 700]

このエンドポイントにトラフィックを送信すると、アプリをスケールアップするためのオートスケーラーがトリガーされます。 http://payment-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN }}[Open URL^] をクリックして、支払いサービスを _trigger_ します。これにより、いくつかのダミーデータが `payment` サービスに送信されますが、より重要なのは、knative が自動的にポッドを再びスピンアップさせ、30秒後にシャットダウンするようにトリガーしたことです。

image::payment-serving-magic.png[serverless, 700]

*おめでとうございます！* これで、支払いサービスが Quarkus ネイティブイメージとしてデプロイされ、_OpenShift Serverless_ で提供され、従来のJavaアプリケーションよりも高速になりました。これでServerlessの能力は終わりではありませんので、次の演習で支払いサービスがどのように _魔法のように_ スケールアップするかを見てみましょう。

それでは、*KafkaSource* を作成して *Knative Eventing* を有効にしてみましょう。このラボでは、_Knative Eventing_ はすでに OpenShift 4 クラスタの _Knative Eventing Operator_ を介してインストールされています。

{{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] に戻って、右上の `+` アイコンをクリックします。

image::plus-icon.png[serverless, 500]

以下の `KafkaSource` を `YAML` エディタでコピーし、*Create* をクリックする :

[source,yaml,role="copypaste"]
----
apiVersion: sources.knative.dev/v1alpha1
kind: KafkaSource
metadata:
  name: kafka-source
spec:
  bootstrapServers:
   - my-cluster-kafka-bootstrap:9092
  topics:
   - orders
  sink:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: payment
----

Kafka と *payments* サービスの間の新しい接続を見ることができます :

[NOTE]
====
Serverless は進化し続けている機能であり、この場合、使用している OpenShift や OpenShift serverless のバージョンによっては、トポロジービューに `KafkaSource` が表示されない場合があります。表示されない場合は心配しないでください。基礎となる技術は期待通りに動作しますので、先に進むだけです。
====

image::kafka-event-source-link.png[serverless, 700]

Coolstore Web UI経由で _Serverless_ の機能で決済サービスが正常に動作するかどうかを確認してみましょう。

=== 4. End to End Functional Testing

始める前に、 {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] で、_payment service_ が再び_ゼロ_にスケールダウンされているかどうかを確認する必要があります :

image::payment-down-again.png[serverless, 700]

ショッピングをしましょう! http://coolstore-ui-{{ USER_ID }}-cloudnativeapps.{{ ROUTE_SUBDOMAIN}}[Red Hat Cool Store^] にアクセスしてください!

以下のショッピングシナリオでは、ショッピングカートにいくつかのクールなアイテムを追加します。:

[arabic]
. *カートに追加* をクリックして、_Forge Laptop Sticker_ をカートに追加してください。トップメニューの下に `Success! トップメニューの下に「追加されました！」というメッセージが表示されます。

image::add-to-cart-serverless.png[serverless, 1000]

[arabic, start=2]
. *Your Shopping Cart* タブに移動し、*Checkout* ボタンをクリックします。クレジットカード情報を入力します。カード情報は16桁で、数字の `4` で始まる必要があります。例えば、`4123987754646678`のように入力します。 

image::checkout-serverless.png[serverless, 1000]

[arabic, start=3]
. クレジットカード情報を入力して、商品代金をお支払いください :

image::input-cc-info-serverless.png[serverless, 1000]

[arabic, start=4]
. _Kafka Event_ がどのようにして _Knative Eventing_ を有効にしているのか確認してみましょう。 {{CONSOLE_URL }}/topology/ns/{{ USER_ID }}-cloudnativeapps[Topology View^] に戻って、*支払いサービス* が自動的に起動しているかどうか確認してください。

image::payment-serving-magic.png[serverless, 500]

[arabic, start=5]
. Confirm the _Payment Status_ of the your shopping items in the *All Orders* tab. It should be `Processing`.

image::payment-processing-serverless.png[serverless, 1000]


[arabic, start=5]
. *All Orders* タブで、ショッピング商品の_お支払い状況を確認してください。`Processing` になっているはずです。

[NOTE]
====
ステータスが *Processing* のままの場合は、注文サービスが受信した Kafka メッセージを処理して MongoDB に保存しています。あと数回ページをリロードしてください。
====

image::payment-completedorfailed-serverless.png[serverless, 1000]

これはこれまでと同じ結果ですが、Knative のイベントを使って、需要に合わせてスケールできる、より強力なイベント駆動型のシステムを作ることができます。

=== 5. Tekton を使用した Cloud-Native CI/CD パイプラインの作成

Cloud Native アプリケーション/マイクロサービスを構築、テスト、デプロイ、管理するためのオープンソース CI/CD ツールは、オンプレミスからプライベート、パブリック、ハイブリッドクラウドまで、数多く存在します。各ツールは、既存のプラットフォーム/システムと統合するためのさまざまな機能を提供しています。そのため、DevOps チームが CI/CD パイプラインを作成し、Kubernetes クラスター上で保守することができなくなることがあります。*Cloud Native CI/CD パイプライン* は、Kubernetes Native の方法で定義し、実行する必要があります。例えば、パイプラインは YAML 形式で Kubernetes リソースとして指定することができます。

*OpenShift Pipelines* は、_Tekton_ をベースにした Kubernetes スタイルの CI/CD ソリューションです。Tekton のビルディングブロックをベースに構築されており、OpenShift や Red Hat の開発者ツールと緊密に統合することで CI/CD 体験を提供します。OpenShift Pipelinesは、CI/CD パイプラインの各ステップを独自のコンテナで実行するように設計されており、各ステップを独立してスケールさせ、以下の機能でパイプラインの要求に対応できるようにしています。

* 標準の Tekton CRD を使用して、コンテナとして実行され、オンデマンドでスケールするパイプラインを定義します。
* チームのデリバリーパイプライン、プラグイン、アクセスコントロールを完全に制御し、中央 CI/CD サーバーで管理する必要はありません。
* OpenShift Console の開発者視点、CLI、IDEによる合理化されたユーザー体験。

image::pipeline-features.png[pipeline, 800]

[NOTE]
====
OpenShift Pipelines プロジェクト は Developer Preview リリースです。開発者プレビューリリースには、完全にテストされていない可能性のある機能が含まれています。お客様は、開発者プレビューリリースを使用してフィードバックを提供することをお勧めします。Red Hat は報告された問題を修正することを約束しておらず、提供された機能は将来のリリースで利用できない可能性があります。
====

ラボでは OpenShift 4 クラスタに OpenShift Pipelines が既にインストールされていますが、自分の OpenShift クラスタに OpenShift Pipelines をインストールしたい場合は、OpenShift OperatorHub で利用できるオペレーターを介してインストールできる OpenShift 上のアドオンとして OpenShift Pipelines が提供されています。

パイプラインを定義するには、以下のように _カスタムリソース_ を作成する必要があります。:

* *Task*: 特定のタスクを実行する再利用可能な疎結合のステップ数(例：コンテナイメージの構築)。
* *Pipeline*: パイプラインの定義と実行すべきタスク
* *PipelineResource*: パイプラインやタスクへの入力 (git リポジトリなど) と出力 (イメージレジストリなど) の出し入れ。
* *TaskRun*: タスクのインスタンスの実行結果 (例 成功 or 失敗)
* *PipelineRun*: パイプラインの実行結果 (例 成功 or 失敗)

image::tekton-arch.png[severless, 800]

パイプラインの概念の詳細については、パイプラインの定義に使用できる様々なパラメータや属性を理解するための優れたガイドを提供している https://github.com/tektoncd/pipeline/tree/master/docs#learn-more[Tekton documentation^] を参照してください。

このラボでは、パイプラインの概念と、OpenShift Serverless プラットフォーム上でマイクロサービスを構築してデプロイするための CI/CD パイプラインの作成と実行方法について説明します。

https://github.com/spring-projects/spring-petclinic[Spring PetClinic^] マイクロサービスアプリを Spring Boot フレームワークを使って `{{ USER_ID }}-cloudnative-pipeline` プロジェクトにデプロイしてみましょう。

OpenShift 上に _PetClinic_ アプリをデプロイするための Kubernetes オブジェクトを作成します。PetClinic アプリ用のコンテナイメージがまだ構築されていないので、デプロイは完了しません。これは以下のセクションで CI/CD パイプラインを介して行います。

*タスク* は、順次実行される複数のステップで構成されています。TaskRun を作成することで _Task_ が実行されます。TaskRun は1つの Pod をスケジュールします。各ステップは、同じポッド内の別のコンテナで実行されます。また、パイプライン内の他のタスクと相互作用するために、入力と出力を持つこともできます。

タスク _task_ が実行を開始すると、ポッドを起動し、同じポッド上の別のコンテナ内で各 *ステップ* を順次実行します。このタスクはたまたま単一のステップを持っていますが、タスクは複数のステップを持つことができ、同じポッド内で実行しているので、ファイルのキャッシュや configmaps、secret などにアクセスするために同じボリュームにアクセスすることができます。タスク `Tasks` は入力 (git リポジトリなど) と出力 (レジストリのイメージなど) を受け取って相互にやりとりすることもできます。

[NOTE]
====
git リポジトリの要件だけがタスクに宣言されていて、使用する特定の git リポジトリを指定しているわけではありません。これにより、_tasks_ を複数のパイプラインや目的に合わせて再利用できるようになります。再利用可能な _task_ の例は https://github.com/tektoncd/catalog[Tekton Catalog^] や https://github.com/openshift/pipelines-catalog[OpenShift Catalog^] リポジトリにあります。
====

Java アプリケーションをコンパイルしてデプロイする方法を知っている、パイプラインで使用する2つの定義済みの Tekton タスクを作成する必要があります。以下のコマンドを使って apply-manifests と update-deployment Tekton タスクをインストールします :

[source,sh,role="copypaste"]
----
oc project user1-cloudnative-pipeline &&
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/apply_manifests_task.yaml && oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m4-labs/payment-service/knative/pipeline/update_deployment_task.yaml
----

すでに CodeReady Workspaces にインストールされている https://github.com/tektoncd/cli/releases[Tekton CLI^] を使って、*tasks* が正しくインストールされているか確認してみましょう :

[source,sh,role="copypaste"]
----
tkn task list
----

以下の二つのタスクを確認できます :

[source,sh]
----
NAME               AGE
apply-manifests   10 seconds ago
update-deployment 10 seconds ago
----

パイプラインは、実行されるべきタスクの数と、それらの入力と出力を介した相互作用の方法を定義します。

このラボでは、PetClinic アプリケーションのソースコードを GitHub から取得し、 https://docs.openshift.com/container-platform/latest/builds/understanding-image-builds.html#build-strategy-s2i_understanding-image-builds[Source-to-Image(S2I)^] を使って OpenShift 上にビルドしてデプロイするパイプラインを作成します。

image::pipeline-diagram.png[serverless, 700]

このパイプラインは以下のことを実行します:

* Git リポジトリ (_app-git_ リソース) からアプリケーションのソースコードをクローンします。
* アプリケーション用の Dockerfile を生成する s2i-java-8 タスクを使用してコンテナイメージをビルドし、 https://buildah.io/[Buildah^] を使用してイメージをビルドします。
* アプリケーションイメージはイメージレジストリにプッシュされます (_app-image_ リソース)
* 新しいアプリケーションイメージは、_openshift-cli_ を使用して OpenShift 上にデプロイされます。

[NOTE]
====
PetClinic の Git リポジトリとそのイメージがレジストリにないことにお気づきかもしれません。これは、Tekton の _Pipelines_ は、アプリケーションのライフサイクルを通して、環境やステージを超えて再利用できるように設計されているからです。
====

パイプラインは、*resources* として生成される Git ソースリポジトリやイメージの詳細を抽象化します。パイプラインをトリガーする際には、パイプラインの実行中に使用する Git リポジトリやイメージのレジストリを指定することができます。我慢してください! これについては次のセクションで少し説明します。

_tasks_ の実行順序は、*inputs* と *outputs* で定義されたタスク間の依存関係と、*runAfter* で定義された明示的な順序によって決定されます。

OpenShift コンソールで右上の `+` ボタンをクリックし、以下の YAML を貼り付けて *Create* をクリックします :

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: build-and-deploy
spec:
  resources:
  - name: git-repo
    type: git
  - name: image
    type: image
  params:
  - name: deployment-name
    type: string
    description: name of the deployment to be patched
  tasks:
  - name: build-image
    taskRef:
      name: buildah
      kind: ClusterTask
    resources:
      inputs:
      - name: source
        resource: git-repo
      outputs:
      - name: image
        resource: image
    params:
    - name: TLSVERIFY
      value: "false"
  - name: apply-manifests
    taskRef:
      name: apply-manifests
    resources:
      inputs:
      - name: source
        resource: git-repo
    runAfter:
    - build-image
  - name: update-deployment
    taskRef:
      name: update-deployment
    resources:
      inputs:
      - name: image
        resource: image
    params:
    - name: deployment
      value: $(params.deployment-name)
    runAfter:
    - apply-manifests
----

作成したパイプラインが表示されます :

image::console-import-yaml-2.png[serverless, 800]

パイプラインが作成されたので、パイプラインで指定したタスクを実行するためにパイプラインを起動します。まず、実行時にパイプラインで使用する Git リポジトリとイメージレジストリの仕様を含む *PipelineResources* をいくつか作成します。これらは複数のパイプラインにまたがって再利用可能です。

以下の *PipelineResource* はフロントエンドアプリケーションの Git リポジトリです。OpenShift コンソールの右上隅にある `+` アイコンをクリックします。以下の`ui-repo` pipelineResource を `YAML` エディタでコピーし、*Create* をクリックします :

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: ui-repo
spec:
  type: git
  params:
  - name: url
    value: http://github.com/openshift-pipelines/vote-ui.git
----

そして以下は、プッシュ先のフロントエンドイメージの OpenShift 内部イメージレジストリを定義しています。ここでも右上隅の+アイコンをクリックします。 `YAML` エディタで以下の `ui-image` pipelineResource をコピーし、 *Create* をクリックします :

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: ui-image
spec:
  type: image
  params:
  - name: url
    value: image-registry.openshift-image-registry.svc:5000/user1-cloudnative-pipeline/vote-ui:latest
----

そして、以下の `api-repo` PipelineResource がバックエンドアプリケーションの git リポジトリを `YAML` エディタで定義しているので、*Create* をクリックします :

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: api-repo
spec:
  type: git
  params:
  - name: url
    value: http://github.com/openshift-pipelines/vote-api.git
----

そして、以下の `api-image` PipelineResourceは、`YAML` エディタでプッシュするバックエンドイメージの OpenShift 内部イメージレジストリを定義し、 *Create* をクリックします :

[source,yaml,role="copypaste"]
----
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: api-image
spec:
  type: image
  params:
  - name: url
    value: image-registry.openshift-image-registry.svc:5000/user1-cloudnative-pipeline/vote-api:latest
----


正常に作成されていることを確認するために、CodeReady Workspaces Terminal で以下の _Tekton CLI_ を実行します :

[source,sh,role="copypaste"]
----
tkn resource ls
----

作成されたリソースの一覧を見ることができます :

[source,shell]
----
NAME        TYPE    DETAILS
api-repo    git     url: http://github.com/openshift-pipelines/vote-api.git
ui-repo     git     url: http://github.com/openshift-pipelines/vote-ui.git
api-image   image   url: image-registry.openshift-image-registry.svc:5000/user1-cloudnative-pipeline/vote-api:latest
ui-image    image   url: image-registry.openshift-image-registry.svc:5000/user1-cloudnative-pipeline/vote-ui:latest
----

*PipelineRun* は、パイプラインを起動して、この特定の呼び出しに使用するべき Git やイメージリソースに結びつける方法です。では、 {{ CONSOLE_URL }}/k8s/ns/{{ USER_ID }}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] に移動して、*Start* をクリックしてみましょう :

image::pipeline-start.png[serverless, 800]

このダイアログボックスでは、_build_ ステップのソースレポの最終ターゲット値と、_deploy_ ステップでデプロイするイメージの名前をバインドします。デフォルト値が表示されているので、*Start* をクリックするだけです :

* deployment-name: `vote-api`
* git-repo: `http://github.com/openshift-pipelines/vote-api.git (api-repo)`
* image: `image-registry.openshift-image-registry.svc:5000/user1-cloudnative-pipeline/vote-api:latest (api-image)`

image::pipeline-start-popup.png[serverless, 700]

*build-and-deploy* を起動するとすぐに pipelinerun がインスタンス化され、パイプラインで定義されているタスクを実行するためのポッドが作成されます。数分後、パイプラインは正常に終了するはずです。ステップの上にカーソルを置くと、ステップの進捗状況を簡単にスナップショットで見ることができますし、ステップをクリックするとステップの詳細なログを見ることができます。

それでは、`vote-ui` アプリケーションをデプロイするためのパイプラインを実行してみましょう。 {{CONSOLE_URL }}/k8s/ns/{{ USER_ID }}}-cloudnative-pipeline/tekton.dev%7Ev1beta1%7EPipeline[OpenShift Pipelines^] に戻って、 *Start* をクリックします。次にパラメータを入力し、以下のようにリソースを選択して、 *Start* をクリックします。

 * deployment-name: `vote-ui`
 * git-repo: `http://github.com/openshift-pipelines/vote-ui.git (api-repo)`
 * image: `image-registry.openshift-image-registry.svc:5000/{{ USER_ID }}-cloudnative-pipeline/vote-ui:latest (api-image)`



image::pipeline-complete.png[serverless, 800]

ビルドが完了したら、 {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}/cloudnative-pipeline[Topology View^] 上で、*Open URL* をクリックします。PetClinic イメージが正常に構築され、デプロイされたことが確認できるはずです。

image::petclinic-openurl.png[serverless, 800]

=== 概要

このモジュールでは、複数 のJava ランタイム(Quarkus と Spring Boot）、Javascript(Node.js)、異なるデータソース(PostgreSQL、MongoDB など)を使用して Cloud Native アプリケーションを開発し、REST API を使用したリアルタイムの _request/response_ 通信、_Red Hat Data Grid_ を使用した高パフォーマンスのキャッシュ可能なサービス、_Red Hat AMQ Streams_ で Apache Kafka を使用したイベントドリブン/リアクティブなショッピングカートサービスを実装した様々なビジネスユースケースに対応する方法を学びました。最後に、Knative で _OpenShift Serverless_ を利用して決済サービスを*Serverless* アプリケーションに変換しました。

*Red Hat Runtimes* は、企業の開発者が高度なクラウドネイティブアーキテクチャを設計し、*Red Hat OpenShift Container Platform* 上のハイブリッドクラウド上でクラウドネイティブアプリケーションの開発、構築、デプロイを行うことを可能にします。おめでとうございます！

==== Additional Resources:

* https://learn.openshift.com/middleware/courses/middleware-quarkus/[Quarkus Tutorials Right in Your Browser^]
* https://developers.redhat.com/articles/quarkus-quick-start-guide-kubernetes-native-java-stack/[Quarkus Quickstart Guide]
* https://docs.openshift.com/container-platform/latest/serverless/serverless-getting-started.html[Getting started with OpenShift Serverless^]
* https://www.openshift.com/learn/topics/pipelines[Cloud-native CI/CD on OpenShift^]
* https://developers.redhat.com/topics/serverless-architecture/[Serverless Architecture Articles^]
